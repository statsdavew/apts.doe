---
title: "APTS Design of Studies - Practical 2 Example Solutions"
subtitle: "September 2020"
author: "Dave Woods (<D.Woods@southampton.ac.uk> ; <http://www.southampton.ac.uk/~davew>) <br> Statistical Sciences Research Institute, University of Southampton"
output: 
#beamer_presentation
pdf_document:
#  html_document:
    theme: null
    highlights: null
bibliography: ../notes/apts_doe.bib
#csl: american-statistical-association.csl
number_sections: true
---

\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bvarepsilon}{\boldsymbol{\varepsilon}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\rT}{\mathrm{T}}
\newcommand{\Var}{\operatorname{Var}}


<style>
pre {
  font-size: 15px;
}
</style>

```{r, echo=FALSE}
options(width=80)
library(knitr)
library(xtable)
set.seed(1)
knit_hooks$set(no.main = function(before, options, envir) {
    if (before) par(mar = c(4.1, 4.1, 1.1, 1.1), pty = "s")  # smaller margin on top
})
opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

<!-- pseudo-Bayesian design using optim-->
1\. **Bayesian $D$-optimal design using <tt>optim</tt>**    
Firstly, we need a function to calculate the expected response for the log-linear model. We can edit the functions from the notes.

```{r loglin}
mu <- function(x, beta0 = 0, beta1 = 1) {
  eta <- beta0 + beta1 * x
  exp(eta)
}
Minfo <- function(xi, beta0 = 0, beta1 = 1) {
  X <- cbind(c(1, 1), xi)
  v <- function(x) mu(x, beta0, beta1)
  W <- diag(c(v(xi[1]), v(xi[2])))
  t(X) %*% W %*% X
}
Dcrit <- function(xi, beta0 = 0, beta1 = 1) {
  d <- det(Minfo(xi, beta0, beta1))
  ifelse(is.nan(d), -Inf, d)
}  
```

Secondly, we will use <tt>optim</tt> to find designs for six values of $\beta_1$. To restrict $-1\le x\le 1$ we use the <tt>"L-BFGS-B"</tt> method and specify lower and upper bounds. We also plot the results.
```{r loglinplot}
xi.opt <- list()
par(mfrow = c(3, 2))
for(beta1 in 1:6) {
  dopt <- optim(par = c(-1, 1), Dcrit, control = list(fnscale = -1), method = "L-BFGS-B", 
                lower = c(-1, -1), upper = c(1, 1), beta1 = beta1)
  xi.opt[[beta1]] <- dopt$par
  curve(mu(x, beta1 = beta1), from = -1, to = 1, ylim = c(0, mu(1, beta1 = beta1)), 
        ylab = "Expected value")
  points(xi.opt[[beta1]], c(0, 0), pch = 16)
}
xi.opt
```

Notice how the second design point is always at $+1$. The location of the first design point varies with $\beta_1$, increasing as $\beta_1$ increases. This pattern makes sense - for larger $\beta_1$, small values of $x_1$ result in $\mu = 0$. As the variance is equal to the mean for this model, running a small design point will provide us with no information (we **know** the response will be zero).

Using the hint, let's fit a regression model to relate the location of this point to the reciprocal of $\beta_1$ to identify the relationship.

```{r theory}
xi2 <- matrix(unlist(xi.opt), nrow = 6, byrow = T)
beta1 <- 1/(1:6)
test.lm <- lm(xi2[ ,1] ~ beta1)
summary(test.lm)
```
We see an exact relationship $x_1 = 1 - 2/\beta_1$. In fact, we can prove for this model that this is the optimal design, $x_1 = 1 - 2/\beta_1$, $x_2 = 1$; see @RWLE2009 

To find the Bayesian $D$-optimal design we define the prior distribution as in the notes, and set up a starting design with the two extremes.
```{r aceglm}
library(acebayes)
prior <- list(support = matrix(c(0, 0, 1, 6), ncol = 2))
d <- matrix(c(-1, 1), nrow = 2)
colnames(d) <- "x"
xi.ace <- aceglm(formula = ~ x, family = poisson, start.d = d, prior = prior, N2 = 0) 
xi.ace$phase2.d
```
The optimal design includes $x_2 = 1$, with $x_1$ having a value nearer the upper end of the design region (probably not surprising, as this value will provide some information for all values of $\beta_1$).


<!-- pseudo-Bayesian design using ace -->
2\. **Bayesian $D$-optimal design using <tt>acebayes</tt>**  
This is a (reasonably) straightforward application of <tt>acenlm</tt>. We define an initial random design in $[0,24]$, and define the uniform prior distribution via a matrix specifying the range for each parameter. The key difference from our use of <tt>aceglm</tt> above is that the columns of the matrix <tt>prior</tt> need to be named for the parameters in the model.

```{r acedes}
n <- 18
start.d <- matrix(runif(n)* 24, ncol = 1)
colnames(start.d) <- c("t")

a1 <- c(0.01884, 0.298)
a2 <- c(0.09884, 8.298)
prior <- list(support = cbind(rbind(a1, a2), c(21.8, 21.8)))
colnames(prior[[1]]) <- c("theta1", "theta2", "theta3")

exp.ace <- acenlm(formula = ~ theta3 * (exp( - theta1 * t) - exp( - theta2 * t)), start.d = start.d, 
                  prior = prior, lower = 0, upper = 24)
sort(exp.ace$phase2.d)
```
Notice the replication (or near replication) in the design. By plotting the design and realisations from the model, we see that the points near where the curves are changing fastest and near the end of the range of $x$.

```{r compplot}
comp <- function(x, theta) theta[3] * (exp(-theta[1] * x) - exp(-theta[2] * x))
M <- 100
par(mfrow = c(1, 1), mar = c(6, 4, 0, 1) + .1)
res <- lapply(1:M, function(l) {
  thetat <- c(runif(1, a1[1], a2[1]), runif(1, a1[2], a2[2]), 21.8)
  curve(comp(x, theta = thetat), from = 0, to = 24, ylab = "Expected concentration", 
        xlab = "Time", ylim = c(0, 25), xlim = c(0, 24), add = l!=1) })
points(exp.ace$phase2.d, rep(0, 18))
```

## References
